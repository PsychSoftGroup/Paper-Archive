{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:30:46.179479Z",
     "start_time": "2025-11-01T18:30:46.175303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# è¯´æ˜ï¼š\n",
    "# - æ•°æ®ï¼šç©ºæ ¼åˆ†è¯ï¼ˆword-levelï¼‰ï¼Œä¿ç•™ <|endoftext|>ï¼Œæ—  PADï¼ˆå›ºå®šé•¿åº¦åˆ‡å—ï¼Œå°¾æ®µä¸¢å¼ƒï¼‰ã€‚\n",
    "# - æ¨¡å‹ï¼šPositionEncoding + [Pre-LNâ†’Self-Attnâ†’Residualâ†’Pre-LNâ†’FFNâ†’Residual] Ã— N + çº¿æ€§è¾“å‡ºå¤´ã€‚\n",
    "# - è®­ç»ƒï¼šLightningï¼Œé»˜è®¤æ··åˆç²¾åº¦ï¼ˆGPU ä¸Šï¼‰ï¼Œå¯è®¾ç½® max_stepsï¼›æ¨èä¸¤é˜¶æ®µï¼ˆå­é›†â†’å…¨é‡ï¼‰ã€‚\n",
    "# - æ¨ç†ï¼šgreedy æˆ– top-k + temperatureï¼Œé‡åˆ° <|endoftext|> æˆ–é•¿åº¦ä¸Šé™åœæ­¢ã€‚\n",
    "#\n",
    "# å°è´´å£«ï¼š\n",
    "# - ä¸ºå…¼å®¹ä½ ç°æœ‰ training_step çš„å†™æ³•ï¼ŒDataLoader é»˜è®¤ batch_size=1ï¼ˆå¯ä¿æŒæœ€å°æ”¹åŠ¨ï¼‰ã€‚\n",
    "# - Notebook ç¯å¢ƒå»ºè®® DataLoader çš„ num_workers=0ï¼Œé¿å…å¤šè¿›ç¨‹å…¼å®¹æ€§é—®é¢˜ã€‚\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "# å»ºè®®å¯ç”¨ Ampere+ çš„ Tensor Core åŠ é€Ÿï¼ˆåœ¨åˆé€‚ç¡¬ä»¶ä¸Šå¯æå‡ matmul æ€§èƒ½ï¼‰\n",
    "torch.set_float32_matmul_precision('high')"
   ],
   "id": "fe97cfa439306dfd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T19:28:14.571642Z",
     "start_time": "2025-11-01T19:28:14.557761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#####################################\n",
    "# æ•°æ®ä¸åˆ†è¯ï¼šç©ºæ ¼åˆ†è¯ï¼Œæ„å»ºè¯è¡¨ä¸æ ·æœ¬\n",
    "#####################################\n",
    "\n",
    "def read_text(path: str, subset_bytes: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    è¯»å– UTF-8 æ–‡æœ¬ã€‚\n",
    "    - subset_bytes: è‹¥ >0ï¼Œä»…è¯»å–å‰ N å­—èŠ‚ï¼ˆç”¨äºâ€œæµ‹è¯•ç‰ˆâ€å¿«é€Ÿè®­ç»ƒéªŒè¯ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read(subset_bytes) if (subset_bytes is not None and subset_bytes > 0) else f.read()\n",
    "    return data.decode('utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def tokenize_by_space(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    ç©ºæ ¼åˆ†è¯ï¼ˆæœ€ç®€å®ç°ï¼‰ï¼š\n",
    "    - ç›´æ¥ä½¿ç”¨ str.split()ï¼›\n",
    "    - <|endoftext|> ä¼šä½œä¸ºæ™®é€š token ä¿ç•™ï¼›\n",
    "    - æ ‡ç‚¹ä¼šä¸è¯åŒå±ä¸€ä¸ª tokenï¼ˆä¾‹å¦‚ \"time,\"ï¼‰ï¼Œä¾¿äºåç»­ C ç«¯å®ç°å¤ç°ã€‚\n",
    "    \"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def build_vocab(tokens: List[str], vocab_size: int, force_tokens: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    æŒ‰è¯é¢‘æ„å»ºè¯è¡¨ï¼š\n",
    "    - force_tokensï¼ˆå¦‚ [\"<|endoftext|>\", \"<UNK>\"]ï¼‰ä¼˜å…ˆæ”¾å…¥è¯è¡¨ï¼›\n",
    "    - å…¶ä½™æŒ‰é¢‘æ¬¡ä»é«˜åˆ°ä½åŠ å…¥ï¼Œç›´åˆ° vocab_sizeã€‚\n",
    "    è¿”å›ï¼š\n",
    "    - token_to_id: è¯ â†’ id\n",
    "    - id_to_token: id â†’ è¯\n",
    "    \"\"\"\n",
    "    counts = Counter(tokens)\n",
    "    vocab: List[str] = []\n",
    "    seen = set()\n",
    "    # 1) ç‰¹æ®Šè¯ä¼˜å…ˆ\n",
    "    for t in force_tokens:\n",
    "        if t not in seen:\n",
    "            vocab.append(t); seen.add(t)\n",
    "    # 2) æŒ‰é¢‘æ¬¡è¡¥è¶³\n",
    "    for tok, _ in counts.most_common():\n",
    "        if tok in seen:\n",
    "            continue\n",
    "        vocab.append(tok); seen.add(tok)\n",
    "        if len(vocab) >= vocab_size:\n",
    "            break\n",
    "    token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
    "    id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "    return token_to_id, id_to_token\n",
    "\n",
    "\n",
    "def encode_tokens(tokens: List[str], token_to_id: Dict[str, int], unk_token: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    å°† token åˆ—è¡¨ç¼–ç ä¸º id åºåˆ—ï¼›æœªçŸ¥è¯æ˜ å°„åˆ° <UNK>ã€‚\n",
    "    \"\"\"\n",
    "    unk_id = token_to_id[unk_token]\n",
    "    return [token_to_id.get(t, unk_id) for t in tokens]\n",
    "\n",
    "\n",
    "def build_fixed_length_dataset(ids: List[int], seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    å›ºå®šé•¿åº¦åˆ‡å—ï¼ˆæ— é‡å ã€æ—  PADï¼‰ï¼š\n",
    "    - å¯¹é•¿åº¦ L çš„ id åºåˆ—ï¼Œå–å‰ floor(L/seq_len)*seq_len éƒ¨åˆ†å¹¶ view æˆ [N, seq_len]ï¼›\n",
    "    - æ¯å— Xï¼ˆé•¿åº¦ seq_lenï¼‰æ„é€  (inp=X[:-1], lbl=X[1:])ï¼Œå³å³ç§»ä¸€ä½ï¼›\n",
    "    - ä¸¢å¼ƒä¸è¶³ seq_len çš„å°¾å·´ï¼Œé¿å…å¼•å…¥ PAD ä¸å¤æ‚ maskã€‚\n",
    "    è¿”å›ï¼š\n",
    "    - inputs: [N, seq_len-1]\n",
    "    - labels: [N, seq_len-1]\n",
    "    \"\"\"\n",
    "    total = len(ids)\n",
    "    usable = (total // seq_len) * seq_len\n",
    "    if usable < seq_len:\n",
    "        raise ValueError(\"è¯­æ–™è¿‡çŸ­ï¼šæ— æ³•æ„é€ ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå—ã€‚è¯·å¢å¤§è¯­æ–™æˆ–é™ä½ seq_lenã€‚\")\n",
    "    arr = torch.tensor(ids[:usable], dtype=torch.long)\n",
    "    blocks = arr.view(-1, seq_len)          # [N, seq_len]\n",
    "    inputs = blocks[:, :-1].contiguous()    # [N, seq_len-1]\n",
    "    labels = blocks[:, 1:].contiguous()     # [N, seq_len-1]\n",
    "    return inputs, labels"
   ],
   "id": "a239fdd67867365",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:34:51.587167Z",
     "start_time": "2025-11-01T18:34:51.573116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#####################################\n",
    "# æ¨¡å‹ï¼šä½ç½®ç¼–ç ã€æ³¨æ„åŠ›ã€FFNã€Blockã€æ•´æ¨¡\n",
    "#####################################\n",
    "\n",
    "class PositionEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆç»å¯¹ä½ç½®ï¼‰æŸ¥è¡¨ï¼š\n",
    "    - é¢„å…ˆè®¡ç®— pe[max_len, d_model]ï¼›\n",
    "    - å‰å‘æ—¶æŒ‰åºåˆ—é•¿åº¦åˆ‡ç‰‡ä¸åµŒå…¥ç›¸åŠ ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=128, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)                            # [pos, dim]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # [pos, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´\n",
    "        self.register_buffer('pe', pe)  # bufferï¼šéšæ¨¡å‹ç§»åŠ¨è®¾å¤‡ï¼Œä½†ä¸å‚ä¸æ¢¯åº¦æ›´æ–°\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [seq, d_model] æˆ– [batch, seq, d_model]\n",
    "        - æœ¬å®ç°é»˜è®¤ batch_size=1ï¼Œå¸¸ç”¨ [seq, d_model]ã€‚\n",
    "        \"\"\"\n",
    "        if x.dim() == 2:\n",
    "            return x + self.pe[:x.size(0), :]\n",
    "        elif x.dim() == 3:\n",
    "            seq_len = x.size(1)\n",
    "            return x + self.pe[:seq_len, :].unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(\"PositionEncoding ä»…æ”¯æŒ 2D æˆ– 3D å¼ é‡ã€‚\")\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    å•å¤´ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆæ— åç½® Wq/Wk/Wvï¼Œè´´è¿‘åŸè®ºæ–‡è®¾ç½®ï¼‰ã€‚\n",
    "    çº¦å®šè¾“å…¥å½¢çŠ¶ä¸º [seq, d_model]ï¼Œä¾¿äºå¤ç”¨ä½ ä¹‹å‰çš„ä»£ç ä¸è®­ç»ƒé€»è¾‘ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=128):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # x: [seq, d_model]\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(self.row_dim, self.col_dim))     # [seq, seq]\n",
    "        scaled = sims / (k.size(self.col_dim) ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: True è¡¨ç¤ºå±è”½ï¼ˆèµ‹æå°å€¼ï¼‰ï¼Œä¸‹ä¸‰è§’å¯è§ï¼Œä¸Šä¸‰è§’å±è”½\n",
    "            masked_value = torch.finfo(scaled.dtype).min\n",
    "            scaled = scaled.masked_fill(mask, masked_value)\n",
    "\n",
    "\n",
    "        weights = F.softmax(scaled, dim=self.col_dim)                       # [seq, seq]\n",
    "        out = torch.matmul(weights, v)                                      # [seq, d_model]\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    æœ€å°å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼šLinear(d_modelâ†’d_ff) + GELU + Linear(d_ffâ†’d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=128, d_ff=512):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    æœ€å° GPT å—ï¼š\n",
    "    - Pre-LN + Self-Attn + æ®‹å·®\n",
    "    - Pre-LN + FFN + æ®‹å·®\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=128, d_ff=512):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = Attention(d_model=d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        a = self.attn(self.ln1(x), mask=mask)\n",
    "        x = x + a\n",
    "        f = self.ffn(self.ln2(x))\n",
    "        x = x + f\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    \"\"\"\n",
    "    å•å¤´ã€å¤šå±‚å †å çš„ Decoder-Only Transformerï¼ˆé€‚åˆå°æ•…äº‹ç”Ÿæˆï¼Œæ˜“äº C ç«¯å¤ç°ï¼‰ã€‚\n",
    "    è¾“å…¥ï¼štoken_ids [seq]ï¼ˆbatch_size=1ï¼‰\n",
    "    è¾“å‡ºï¼šlogits [seq, vocab_size]\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, n_layers: int = 2, d_ff: int = 512, max_len: int = 512, lr: float = 3e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        L.seed_everything(42)\n",
    "\n",
    "        self.we = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model=d_model, d_ff=d_ff) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # token_ids: [seq]\n",
    "        x = self.we(token_ids)     # [seq, d_model]\n",
    "        x = self.pe(x)             # å åŠ ä½ç½®ç¼–ç \n",
    "\n",
    "        # å› æœæ©ç ï¼šä¸‹ä¸‰è§’ä¸ºå¯è§ï¼ˆFalseï¼‰ï¼Œä¸Šä¸‰è§’å±è”½ï¼ˆTrueï¼‰\n",
    "        seq_len = token_ids.size(0)\n",
    "        mask = torch.tril(torch.ones((seq_len, seq_len), device=self.device, dtype=torch.bool))\n",
    "        mask = ~mask  # True è¡¨ç¤ºå±è”½\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)      # [seq, vocab]\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # å°æ¨¡å‹ä¸‹æ— éœ€å¤æ‚è°ƒåº¦å™¨ï¼›AdamW + é€‚åº¦æƒé‡è¡°å‡è¶³ä»¥æ”¶æ•›\n",
    "        return AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Notebook æœ€å°æ”¹åŠ¨ï¼šä¿æŒ batch_size=1ï¼Œå– batch ä¸­ç¬¬ 0 ä¸ªæ ·æœ¬ã€‚\n",
    "        è‹¥æœªæ¥æ”¹ä¸º batch_size>1ï¼Œéœ€è¦ä¿®æ”¹è¿™é‡Œå»æ‰ [0] å¹¶é€‚é…å‰å‘å½¢çŠ¶ã€‚\n",
    "        \"\"\"\n",
    "        inputs, labels = batch          # [1, seq] / [1, seq]\n",
    "        inputs = inputs[0]              # [seq]\n",
    "        labels = labels[0]              # [seq]\n",
    "        logits = self.forward(inputs)   # [seq, vocab]\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss"
   ],
   "id": "3f68da638f93b552",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:30:57.040773Z",
     "start_time": "2025-11-01T18:30:57.031997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#####################################\n",
    "# é‡‡æ ·ä¸æ¨ç†ï¼ˆgreedy / top-k + temperatureï¼‰\n",
    "#####################################\n",
    "\n",
    "def sample_next_id(logits_row: torch.Tensor, temperature: float = 1.0, top_k: Optional[int] = None) -> int:\n",
    "    \"\"\"\n",
    "    ä»å•æ­¥ logitsï¼ˆå½¢çŠ¶ [vocab]ï¼‰ä¸­é‡‡æ ·ä¸‹ä¸€ä¸ª token idã€‚\n",
    "    - temperature: >1 æ›´å‘æ•£ï¼Œ<1 æ›´ä¿å®ˆï¼›=1 ä¸å˜\n",
    "    - top_k: ä»…åœ¨å‰ top_k é‡Œé‡‡æ ·ï¼ˆå¯å‡å°‘ä½æ¦‚ç‡å™ªå£°ï¼‰ï¼ŒNone/<=0 è¡¨ç¤ºå…³é—­\n",
    "    \"\"\"\n",
    "    if temperature != 1.0:\n",
    "        logits_row = logits_row / temperature\n",
    "\n",
    "    if top_k is not None and top_k > 0:\n",
    "        k = min(top_k, logits_row.size(0))\n",
    "        top_vals, top_idx = torch.topk(logits_row, k=k)\n",
    "        filt = torch.full_like(logits_row, float('-inf'))\n",
    "        filt[top_idx] = top_vals\n",
    "        logits_row = filt\n",
    "\n",
    "    probs = F.softmax(logits_row, dim=-1)\n",
    "    next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "    return next_id\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    model: DecoderOnlyTransformer,\n",
    "    token_to_id: Dict[str, int],\n",
    "    id_to_token: Dict[int, str],\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    eos_token: str = \"<|endoftext|>\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    è‡ªå›å½’ç”Ÿæˆï¼ˆbatch_size=1ï¼‰ï¼š\n",
    "    - é»˜è®¤ç”¨é‡‡æ ·ï¼ˆtemperature/top_k å¯æ§ï¼‰ï¼›è‹¥æƒ³ greedyï¼Œåˆ™è®¾ç½® temperature=1 ä¸” top_k=Noneï¼Œ\n",
    "      å¹¶å¯å°† sample_next_id æ”¹æˆ argmaxã€‚\n",
    "    - ç¢°åˆ° <|endoftext|> æå‰åœæ­¢ï¼Œæˆ–è¾¾åˆ° max_new_tokens åœæ­¢ã€‚\n",
    "    \"\"\"\n",
    "    unk = \"<UNK>\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        # 1) ç¼–ç  prompt\n",
    "        prompt_ids = encode_tokens(tokenize_by_space(prompt), token_to_id, unk)\n",
    "        if len(prompt_ids) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)  # [seq0]\n",
    "\n",
    "        # 2) é€ token ç”Ÿæˆ\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(x)             # [seq, vocab]\n",
    "            last = logits[-1]             # æœ€è¿‘ä¸€æ­¥çš„ logits\n",
    "            next_id = sample_next_id(last, temperature=temperature, top_k=top_k)\n",
    "            x = torch.cat([x, torch.tensor([next_id], dtype=torch.long, device=device)], dim=0)\n",
    "            if id_to_token.get(next_id, \"\") == eos_token:\n",
    "                break\n",
    "\n",
    "        # 3) ä»…è¿”å›æ–°å¢éƒ¨åˆ†ï¼ˆæ›´ç›´è§‚ï¼‰\n",
    "        gen_ids = x[len(prompt_ids):]\n",
    "        tokens = [id_to_token[i.item()] for i in gen_ids]\n",
    "        return \" \".join(tokens)\n"
   ],
   "id": "ea292b7b09cc7ef1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:30:59.492837Z",
     "start_time": "2025-11-01T18:30:59.486521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#####################################\n",
    "# è®­ç»ƒå‡½æ•°ï¼ˆNotebook å‹å¥½ï¼‰\n",
    "#####################################\n",
    "\n",
    "def run_train(\n",
    "    corpus_path: str,\n",
    "    save_dir: str,\n",
    "    subset_mb: int = 3,         # æµ‹è¯•ç‰ˆé»˜è®¤å–å‰ 3MB\n",
    "    vocab_size: int = 4000,\n",
    "    seq_len: int = 128,\n",
    "    d_model: int = 128,\n",
    "    n_layers: int = 2,\n",
    "    d_ff: int = 512,\n",
    "    max_len: int = 512,\n",
    "    lr: float = 3e-4,\n",
    "    max_steps: int = 500,       # æµ‹è¯•ç‰ˆé»˜è®¤ 500 æ­¥\n",
    "    batch_size: int = 1,        # ä¸ training_step å¯¹é½ï¼ˆæœ€å°æ”¹åŠ¨ï¼‰\n",
    "    num_workers: int = 0,       # Notebook å»ºè®®=0ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜\n",
    "    use_gpu: bool = True,       # è‡ªåŠ¨é€‰æ‹© GPU/CPU ä¸ç²¾åº¦\n",
    "):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒå…¥å£ï¼ˆä¸º Notebook è®¾è®¡ï¼‰ï¼š\n",
    "    - subset_mb>0ï¼šä»…è¯»å–å‰ N MB æ–‡æœ¬ç”¨äºâ€œæµ‹è¯•ç‰ˆâ€è®­ç»ƒï¼›\n",
    "      subset_mb<=0ï¼šè¯»å–å…¨é‡æ–‡æœ¬ç”¨äºâ€œå…¨é‡ç‰ˆâ€è®­ç»ƒã€‚\n",
    "    - è®­ç»ƒç»“æŸä¿å­˜ä¸‰ç±»å·¥ä»¶ï¼švocab.jsonã€config.jsonã€transformer_state.ptï¼ˆåç»­æ¨ç†ä¸ C ç«¯ä¼šç”¨åˆ°ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1) è¯»å–æ–‡æœ¬ï¼ˆå¯é€‰å­é›†ï¼‰\n",
    "    subset_bytes = None if (subset_mb is None or subset_mb <= 0) else subset_mb * 1024 * 1024\n",
    "    text = read_text(corpus_path, subset_bytes=subset_bytes)\n",
    "\n",
    "    # 2) åˆ†è¯ä¸è¯è¡¨\n",
    "    tokens = tokenize_by_space(text)\n",
    "    force_tokens = [\"<|endoftext|>\", \"<UNK>\"]\n",
    "    token_to_id, id_to_token = build_vocab(tokens, vocab_size=vocab_size, force_tokens=force_tokens)\n",
    "\n",
    "    # 3) ç¼–ç ä¸º id åºåˆ—\n",
    "    ids = encode_tokens(tokens, token_to_id, \"<UNK>\")\n",
    "\n",
    "    # 4) æ„é€ å›ºå®šé•¿åº¦æ ·æœ¬ï¼ˆæ— é‡å ã€æ—  PADï¼‰\n",
    "    inputs, labels = build_fixed_length_dataset(ids, seq_len=seq_len)  # æ¯æ¡æ ·æœ¬é•¿åº¦ä¸º seq_len-1\n",
    "    dataset = TensorDataset(inputs, labels)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 5) åˆå§‹åŒ–æ¨¡å‹\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=len(token_to_id),\n",
    "        d_model=d_model,\n",
    "        n_layers=n_layers,\n",
    "        d_ff=d_ff,\n",
    "        max_len=max_len,\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # 6) Lightning Trainerï¼ˆè‡ªåŠ¨è®¾å¤‡ + æ··åˆç²¾åº¦ï¼‰\n",
    "    accelerator = \"gpu\" if (use_gpu and torch.cuda.is_available()) else \"cpu\"\n",
    "    precision = \"16-mixed\" if accelerator == \"gpu\" else \"32-true\"\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_steps=max_steps,\n",
    "        accelerator=accelerator,\n",
    "        devices=1,\n",
    "        precision=precision,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "\n",
    "    # 7) å¼€å§‹è®­ç»ƒ\n",
    "    trainer.fit(model, train_dataloaders=loader)\n",
    "\n",
    "    # 8) ä¿å­˜è¯è¡¨ã€é…ç½®ã€æƒé‡\n",
    "    with open(os.path.join(save_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(token_to_id, f, ensure_ascii=False, indent=2)\n",
    "    config = dict(\n",
    "        vocab_size=len(token_to_id),\n",
    "        d_model=d_model,\n",
    "        n_layers=n_layers,\n",
    "        d_ff=d_ff,\n",
    "        max_len=max_len,\n",
    "        seq_len=seq_len\n",
    "    )\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"transformer_state.pt\"))\n",
    "\n",
    "    return {\n",
    "        \"save_dir\": save_dir,\n",
    "        \"vocab_size\": len(token_to_id),\n",
    "        \"num_samples\": len(dataset),\n",
    "        \"accelerator\": accelerator,\n",
    "        \"precision\": precision\n",
    "    }"
   ],
   "id": "85a2014b6ce1e436",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:31:02.750329Z",
     "start_time": "2025-11-01T18:31:02.746111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#####################################\n",
    "# æ¨ç†å‡½æ•°ï¼ˆNotebook å‹å¥½ï¼‰\n",
    "#####################################\n",
    "\n",
    "def run_generate(\n",
    "    save_dir: str,\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 0,\n",
    "    max_new_tokens: int = 256,\n",
    "    use_gpu: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    æ¨ç†å…¥å£ï¼š\n",
    "    - ä» save_dir è½½å…¥ vocab/config/æƒé‡ï¼›\n",
    "    - åœ¨ CPUï¼ˆé»˜è®¤ï¼‰æˆ– GPU ä¸Šæ„å»ºæ¨¡å‹å¹¶ç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "    è¿”å›ç”Ÿæˆçš„å­—ç¬¦ä¸²ï¼ˆä¸å« promptï¼Œä¾¿äºè§‚å¯Ÿæ–°å¢éƒ¨åˆ†ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    # 1) åŠ è½½è¯è¡¨ä¸é…ç½®\n",
    "    with open(os.path.join(save_dir, \"vocab.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        token_to_id = json.load(f)\n",
    "    # åå‘å­—å…¸ï¼šid -> token\n",
    "    id_to_token = {int(v): k for k, v in token_to_id.items()}\n",
    "\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    # 2) åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=cfg[\"vocab_size\"],\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        d_ff=cfg[\"d_ff\"],\n",
    "        max_len=cfg[\"max_len\"],\n",
    "        lr=3e-4\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if (use_gpu and torch.cuda.is_available()) else \"cpu\"\n",
    "    state = torch.load(os.path.join(save_dir, \"transformer_state.pt\"), map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 3) ç”Ÿæˆ\n",
    "    out = generate_text(\n",
    "        model=model,\n",
    "        token_to_id=token_to_id,\n",
    "        id_to_token=id_to_token,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=(None if top_k <= 0 else top_k),\n",
    "        eos_token=\"<|endoftext|>\"\n",
    "    )\n",
    "    return out"
   ],
   "id": "efe222394e7a5c04",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:06:17.230807Z",
     "start_time": "2025-11-01T20:06:17.226067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_path = \"TinyStories-train.txt\"\n",
    "save_dir = \"runs_ipynb/test\"\n",
    "subset_mb = 1024 #ä»…å–å‰1024MBæ–‡æœ¬\n",
    "vocab_size = 32000\n",
    "seq_len = 512\n",
    "d_model = 512; n_layers = 4; d_ff = 2048; max_len = 1024\n",
    "lr = 2e-4; max_steps = 20000\n",
    "batch_size = 1; num_workers = 0; use_gpu = True"
   ],
   "id": "3abbb4ec4d98ecc5",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:18.593317Z",
     "start_time": "2025-11-01T20:06:18.492682Z"
    }
   },
   "cell_type": "code",
   "source": "info = run_train(corpus_path, save_dir, subset_mb, vocab_size, seq_len, d_model, n_layers, d_ff, max_len, lr, max_steps, batch_size, num_workers, use_gpu)",
   "id": "b6cb2e6933605d3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | we      | Embedding        | 16.4 M | train\n",
      "1 | pe      | PositionEncoding | 0      | train\n",
      "2 | blocks  | ModuleList       | 11.6 M | train\n",
      "3 | ln_f    | LayerNorm        | 1.0 K  | train\n",
      "4 | head    | Linear           | 16.4 M | train\n",
      "5 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "44.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.4 M    Total params\n",
      "177.415   Total estimated model params size (MB)\n",
      "50        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [08:33<00:00, 38.97it/s, v_num=12, train_loss_step=2.920, train_loss_epoch=3.650]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=20000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [08:35<00:00, 38.81it/s, v_num=12, train_loss_step=2.920, train_loss_epoch=3.650]\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:20:12.528579Z",
     "start_time": "2025-11-01T20:20:11.832230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Once upon a time, there was a girl\"\n",
    "print(run_generate(save_dir=\"runs_ipynb/test\", prompt=prompt, temperature=0.9, top_k=50, max_new_tokens=128, use_gpu=False))"
   ],
   "id": "895a5b7dceb6ed75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named amy. amy was three years old. one day, amy was walking through the park with her mom when they saw a swing. amy wanted to get a better look. mama asked her to give the ball away. amy was brave and began to move and she felt happy. the end. <|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T19:55:29.999426Z",
     "start_time": "2025-11-01T19:55:29.391343Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5d2f68fec945710e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --run_dir RUN_DIR\n",
      "ipykernel_launcher.py: error: the following arguments are required: --run_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scheng/github-project/TinyTansformer/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 80
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
