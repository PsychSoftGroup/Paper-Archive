## 摘要
作为人们日常生活中不可或缺的一部分，社交媒体正成为自动化心理健康分析的重要数据源。传统的判别式方法具有较差的泛化能力和较低的可解释性，近年来，大型语言模型（LLMs）已被用于社交媒体上的可解释心理健康分析，旨在在零样本或少样本场景下，除了预测结果，还能提供详细的解释。然而，研究结果显示，LLMs在零样本/少样本方式下的分类性能仍不理想，这也显著影响了生成解释的质量。领域特定的微调是一种有效的解决方案，但面临两个关键挑战：1）缺乏高质量的训练数据；2）没有开源的基础LLM模型。

为缓解这些问题，我们将可解释心理健康分析正式建模为文本生成任务，并构建了首个多任务、多来源的可解释心理健康指令（IMHI）数据集，包含10.5万条数据样本，用以支持LLM的指令微调和评估。原始社交媒体数据来自于覆盖8个心理健康分析任务的10个现有数据源。我们通过专家设计的少样本提示语，对ChatGPT进行提示以获得解释。为确保解释的可靠性，我们对生成数据的正确性、一致性和质量进行了严格的自动化和人工评估。

基于IMHI数据集和LLaMA2基础模型，我们训练了MentaLLaMA，这是首个面向社交媒体可解释心理健康分析的开源指令遵循型LLM系列。我们在IMHI基准上对MentaLLaMA和其他先进方法进行了评测，IMHI是首个针对可解释心理健康分析的全面评测基准。结果表明，MentaLLaMA在正确性上可接近最先进的判别式方法，并且能够生成达到人类水平的解释。MentaLLaMA模型还在未见任务上展现出强大的泛化能力。项目地址为：https://github.com/SteveKGYang/MentaLLaMA。

## 关键词
心理健康分析、可解释性、社交媒体、大型语言模型

### ACM参考格式：
Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, 和 Sophia Ananiadou. 2024. MentaLLaMA: 基于大型语言模型的社交媒体可解释心理健康分析. 载于2024年ACM Web大会论文集（WWW ’24），2024年5月13-17日，新加坡。ACM, 纽约, 美国, 13页。https://doi.org/10.1145/XXXXXX.XXXXXX

## 1 引言
与心理健康相关的问题正对全球公共健康构成日益严重的威胁[9]，但由于社会认知的缺乏和污名化现象的存在，这些问题仍被低估[31]。随着网络技术的发展，社交媒体已经成为人们日常生活中不可或缺的一部分¹。许多有潜在心理健康问题的人会在Twitter和Reddit等社交媒体平台上分享自己的感受，这使得社交媒体文本成为心理健康分析和潜在早期干预的重要数据来源[2, 36]。然而，面对社交媒体帖子数量的爆炸式增长，人工进行心理健康分析已变得不可能。因此，许多研究开始探索利用自然语言处理（NLP）技术在社交媒体上实现自动化心理健康分析[10]。

在心理健康的NLP研究中，以往的方法主要将社交媒体上的心理健康分析建模为文本分类任务，其中预训练语言模型（PLMs）[19]达到了最先进（SOTA）的性能。然而，PLMs在应对未见任务时往往泛化能力较差，并且在多任务场景下缺乏鲁棒性[26, 39]。这些方法的另一个主要局限是，它们做出的判别性预测可解释性较低，限制了其在实际应用中的可靠性。为缓解这些问题，最新的大型语言模型（LLMs），如ChatGPT²和GPT-4[29]，已被用于检测多种心理健康状况并为其决策提供详细解释，因为它们被证明拥有更强的泛化能力[4, 42]。具体来说，Yang等人[45]进行了综合研究和细致的人类评估，表明ChatGPT具有很强的上下文学习能力，并且能够为其正确分类生成接近人类水平的解释，这显示出其提升心理健康分析可解释性的潜力。

然而，像ChatGPT这样的闭源大型语言模型（LLM）在零样本[1]或少样本[44]学习环境下，仍难以达到最先进（SOTA）监督方法在心理健康分类任务上的表现。此外，这种较低的精度被证明会进一步显著影响生成解释的质量，即所谓的不准确推理[45]。一种有效的解决方案是利用任务特定的数据对LLM进行微调[15, 43]，这可以更好地使LLM与目标领域对齐，同时保持较强的泛化能力。然而，在通过微调提升LLM在可解释心理健康分析方面的表现时，存在两个关键挑战。

首先，微调LLM需要高质量的有监督训练数据。在社交媒体上的心理健康分析中，尽管已有一些数据集包含了简短的随意文本片段[11, 12]，但仍缺乏公开的、高质量的能够为检测结果提供详细且可靠解释的数据。这主要是由于该研究主题的敏感性[3, 28]，以及由领域专家撰写解释的高昂成本所致。

其次，对闭源LLM（如ChatGPT）进行提示或微调不仅成本高昂³、耗时长，还伴随着巨大的碳排放⁴，同时，目前尚未有面向可解释心理健康分析的开源LLM对外发布。资源的匮乏和高昂的成本阻碍了相关研究的进展。

首先，微调LLM需要高质量的有监督训练数据。在社交媒体上的心理健康分析任务中，尽管已有少量数据集包含了简短的随意文本片段[11, 12]，但仍然缺乏能够为检测结果提供详细且可靠解释的开源数据。这主要是由于该研究主题的敏感性[3, 28]，以及由领域专家撰写解释所需的高昂成本所致。

其次，对闭源LLM（如ChatGPT）进行提示或微调不仅成本高、耗时长，还伴随着巨大的碳排放，同时，目前尚未有面向可解释心理健康分析的开源LLM对外发布。资源的缺乏和高昂的成本阻碍了相关研究的进展。

为弥补这些不足，我们将可解释心理健康分析正式建模为一个文本生成任务，旨在检测社交媒体帖子中的心理健康状况证据，并为预测结果生成解释。我们构建了首个多任务、多来源的可解释心理健康指令（IMHI）数据集，包含10.5万条数据样本，用以支持LLM的指令微调[30]和评估。

首先，我们从10个现有数据源中收集了原始数据，覆盖了8个心理健康分析任务。所收集的数据包括社交媒体帖子及其对应的心理健康相关任务标注。其次，受self-instruct方法[41]成功经验的启发，并考虑到ChatGPT在为心理健康分析生成接近人类水平解释方面的巨大潜力[45]，我们利用专家编写的少样本示例和收集到的标注，来提示ChatGPT为每个标注生成高质量的解释。为确保解释的质量，我们对所有收集到的数据进行了全面的自动化评估，评估内容包括预测的正确性、标注与解释之间的一致性以及解释的质量。我们还针对部分数据，采用领域专家精心设计的标注方案进行了人工评估。第三，我们将所有收集到的社交媒体帖子、标注和解释，按照规则转换为基于指令的问答对，这些数据用于构建IMHI训练集和IMHI评测基准，这也是首个面向可解释心理健康分析任务的全面评测基准。

基于IMHI数据集，我们提出了MentaLLaMA，这是首个基于LLaMA2基础模型[35]、具备指令遵循能力的开源可解释心理健康分析大模型系列。具体来说，我们微调了三种不同规模的MentaLLaMA模型：MentaLLaMA-7B、MentaLLaMA-chat-7B和MentaLLaMA-chat-13B（图1中展示了MentaLLaMA强大能力的部分示例）。我们在IMHI评测基准上对MentaLLaMA模型及其他先进方法进行了全面评估，重点关注心理健康检测的正确性和生成解释的质量。结果显示，MentaLLaMA-chat-13B在10个测试集中的7个上，其预测正确性超过或接近最先进的判别式方法[19]，且MentaLLaMA生成的解释与ChatGPT相当，始终优于生成式预训练语言模型（PLM）。其生成质量得益于指令微调、人类反馈强化学习（RLHF）[34]以及模型规模的提升。MentaLLaMA模型还展现出对未见任务的强大泛化能力，在预测正确性上优于ChatGPT，在解释质量上也超过生成式PLM。

我们的主要贡献总结如下： 1）我们正式定义了可解释心理健康分析任务，并构建了IMHI数据集，这是第一个面向社交媒体可解释心理健康分析的多任务、多来源指令微调数据集。 2）我们提出了MentaLLaMA，这是首个面向可解释心理健康分析的开源指令遵循大语言模型系列。 3）我们引入了首个包含1.9万测试样本的全面评测基准，覆盖8个任务和10个测试集。 4）我们在该基准上的实验结果和分析表明，MentaLLaMA在预测正确性、生成解释的质量和泛化能力方面均具有明显优势。







[9 Socio-economic variations in the mental health treatment gap for people with anxiety, mood, and substance use disorders: Results from the WHO World Mental Health (WMH) Surveys](https://pmc.ncbi.nlm.nih.gov/articles/PMC6878971/)

[10 Mental Health Analysis in Social Media Posts: A Survey](https://pubmed.ncbi.nlm.nih.gov/36619138/)

[19 MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare](https://aclanthology.org/2022.lrec-1.778/)

[34 Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)

[35 Llama 2: Open Foundation and Fine-Tuned Chat Model](https://arxiv.org/abs/2307.09288)

[41 Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://aclanthology.org/2023.acl-long.754/)

[45 Towards Interpretable Mental Health Analysis with Large Language Models](https://aclanthology.org/2023.emnlp-main.370/)

其他可供参考文章：
[心理所开发出新型的生活满意度评估方法](https://www.psych.ac.cn/news/kyjz/202410/t20241031_7411463.html)

[利用机器学习预测青少年心理健康风险](https://nursing.sysu.edu.cn/article/3089)
